<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-vision-language-action-models" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.3">
<title data-rh="true">Vision-Language-Action Models | Physical AI &amp; Humanoid Robotics: Bridging Digital Intelligence and the Physical World</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://phys-git-main-sairarizvi6-projects.vercel.app/vision-language-action-models"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="chat-api" content="https://phys-chatbot-api.vercel.app/chat"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Vision-Language-Action Models | Physical AI &amp; Humanoid Robotics: Bridging Digital Intelligence and the Physical World"><meta data-rh="true" name="description" content="Bridging Perception, Cognition, and Action with Language"><meta data-rh="true" property="og:description" content="Bridging Perception, Cognition, and Action with Language"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://phys-git-main-sairarizvi6-projects.vercel.app/vision-language-action-models"><link data-rh="true" rel="alternate" href="https://phys-git-main-sairarizvi6-projects.vercel.app/vision-language-action-models" hreflang="en"><link data-rh="true" rel="alternate" href="https://phys-git-main-sairarizvi6-projects.vercel.app/vision-language-action-models" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.0a5f47d9.css">
<link rel="preload" href="/assets/js/runtime~main.a22e79de.js" as="script">
<link rel="preload" href="/assets/js/main.8e1e1db1.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/introduction-to-physical-ai">Textbook</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/panaversity/ai-robotics-textbook" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/introduction-to-physical-ai">Introduction to Physical AI</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/embodied-intelligence-and-humanoids">Embodied Intelligence and Humanoids</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ros2-the-robotic-nervous-system">ROS 2 - The Robotic Nervous System</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/urdf-and-robot-description">URDF and Robot Description</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/sensors-in-physical-ai">Sensors in Physical AI</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/simulation-with-gazebo">Simulation with Gazebo</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/nvidia-isaac-sim-and-digital-twins">NVIDIA Isaac Sim and Digital Twins</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/isaac-ros-and-perception">Isaac ROS and Perception</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/navigation-and-bipedal-locomotion">Navigation and Bipedal Locomotion</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/vision-language-action-models">Vision-Language-Action Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/conversational-robotics-voice-to-action">Conversational Robotics - Voice to Action</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/balance-manipulation-and-whole-body-control">Balance, Manipulation, and Whole-Body Control</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/capstone-autonomous-humanoid">Capstone - Autonomous Humanoid</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/recommended-humanoid-robots">Recommended Humanoid Robots</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/student-hardware-guide">Student Hardware Guide</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/">Welcome to Physical AI &amp; Humanoid Robotics</a></li></ul></nav></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Vision-Language-Action Models</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Chapter 10: Vision-Language-Action Models</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="bridging-perception-cognition-and-action-with-language">Bridging Perception, Cognition, and Action with Language<a href="#bridging-perception-cognition-and-action-with-language" class="hash-link" aria-label="Direct link to Bridging Perception, Cognition, and Action with Language" title="Direct link to Bridging Perception, Cognition, and Action with Language">​</a></h2><p>The ability to understand and execute commands given in natural language, while interpreting visual information from the environment, represents a significant leap towards truly intelligent and intuitive humanoid robots. <strong>Vision-Language-Action (VLA) models</strong> are a new class of AI architectures designed to bridge the gap between disparate modalities—vision, language, and robotic actions—enabling robots to perceive, reason about, and act upon human instructions in complex, unstructured environments. These models are crucial for advancing conversational robotics and general-purpose humanoid intelligence.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-multimodal-challenge-in-robotics">The Multimodal Challenge in Robotics<a href="#the-multimodal-challenge-in-robotics" class="hash-link" aria-label="Direct link to The Multimodal Challenge in Robotics" title="Direct link to The Multimodal Challenge in Robotics">​</a></h3><p>Traditional robotic systems often handle vision, language, and control as separate modules, leading to brittle performance when faced with real-world ambiguity. For example, a robot might &quot;see&quot; a cup but not understand a command like &quot;please bring me the blue mug on the table.&quot; VLA models aim to integrate these modalities, allowing for a more holistic understanding:</p><ul><li><strong>Visual Grounding</strong>: Connecting natural language descriptions (e.g., &quot;blue mug&quot;) to specific objects or regions in the robot&#x27;s visual field.</li><li><strong>Semantic Understanding</strong>: Interpreting the meaning of commands and translating them into a sequence of executable robotic actions.</li><li><strong>Action Generation</strong>: Mapping high-level instructions to low-level motor commands for manipulation, navigation, and interaction.</li></ul><div class="language-mermaid codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-mermaid codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">graph TD</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    A[Natural Language Command] --&gt; B(Language Encoder)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    C[Camera Input] --&gt; D(Vision Encoder)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    B --&gt; E{Multimodal Fusion}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    D --&gt; E</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    E --&gt; F[High-Level Action Planner]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    F --&gt; G[Low-Level Robot Controller]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    G --&gt; H[Robot Actions in Environment]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><em>Figure 10.1: Conceptual architecture of a Vision-Language-Action model.</em></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="foundations-of-vla-models">Foundations of VLA Models<a href="#foundations-of-vla-models" class="hash-link" aria-label="Direct link to Foundations of VLA Models" title="Direct link to Foundations of VLA Models">​</a></h2><p>VLA models draw heavily from advancements in large language models (LLMs) and large vision models (LVMs), extending their capabilities to embodied agents.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="101-large-language-models-llms">10.1. Large Language Models (LLMs)<a href="#101-large-language-models-llms" class="hash-link" aria-label="Direct link to 10.1. Large Language Models (LLMs)" title="Direct link to 10.1. Large Language Models (LLMs)">​</a></h3><p>LLMs excel at understanding and generating human language. In VLA models, they are used to:</p><ul><li><strong>Interpret Commands</strong>: Parse ambiguous natural language instructions into actionable intents.</li><li><strong>Reasoning</strong>: Perform high-level reasoning about tasks, common sense, and causality.</li><li><strong>Dialogue Management</strong>: Engage in conversational turns to clarify ambiguities or provide feedback.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="102-large-vision-models-lvms">10.2. Large Vision Models (LVMs)<a href="#102-large-vision-models-lvms" class="hash-link" aria-label="Direct link to 10.2. Large Vision Models (LVMs)" title="Direct link to 10.2. Large Vision Models (LVMs)">​</a></h3><p>LVMs are powerful image and video processing models. In VLA, they provide:</p><ul><li><strong>Object Recognition</strong>: Identify a vast array of objects in the scene.</li><li><strong>Scene Understanding</strong>: Analyze the context, relationships between objects, and environmental semantics.</li><li><strong>Visual Question Answering (VQA)</strong>: Answer questions about the visual content.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="103-multimodal-fusion">10.3. Multimodal Fusion<a href="#103-multimodal-fusion" class="hash-link" aria-label="Direct link to 10.3. Multimodal Fusion" title="Direct link to 10.3. Multimodal Fusion">​</a></h3><p>The core of VLA models lies in effectively combining the information from language and vision. Techniques include:</p><ul><li><strong>Cross-Attention Mechanisms</strong>: Allowing the vision encoder to focus on parts of an image relevant to a language query, and vice-versa.</li><li><strong>Shared Latent Spaces</strong>: Mapping visual and linguistic features into a common embedding space where their relationships can be learned.</li><li><strong>Transformer Architectures</strong>: Adapting transformer networks to handle multimodal inputs, leveraging their success in both NLP and vision.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="from-language-to-robot-action">From Language to Robot Action<a href="#from-language-to-robot-action" class="hash-link" aria-label="Direct link to From Language to Robot Action" title="Direct link to From Language to Robot Action">​</a></h2><p>The most challenging aspect of VLA models for robotics is translating abstract linguistic and visual understanding into concrete, physical actions.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="104-action-space-and-representation">10.4. Action Space and Representation<a href="#104-action-space-and-representation" class="hash-link" aria-label="Direct link to 10.4. Action Space and Representation" title="Direct link to 10.4. Action Space and Representation">​</a></h3><p>Robotic actions can be represented at various levels of abstraction:</p><ul><li><strong>High-Level Actions</strong>: Symbolic commands like <code>pick_up(object, location)</code> or <code>go_to(room)</code>. These require further decomposition.</li><li><strong>Low-Level Actions</strong>: Direct motor commands (joint velocities, torques, end-effector poses) that the robot can execute. These are often continuous and high-dimensional.</li></ul><p>VLA models often output either high-level action sequences that are then fed into a traditional robot planner, or directly generate low-level control policies through reinforcement learning or imitation learning.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="105-learning-paradigms">10.5. Learning Paradigms<a href="#105-learning-paradigms" class="hash-link" aria-label="Direct link to 10.5. Learning Paradigms" title="Direct link to 10.5. Learning Paradigms">​</a></h3><ul><li><strong>Reinforcement Learning from Human Feedback (RLHF)</strong>: Humans provide feedback on robot actions, guiding the model to learn desired behaviors.</li><li><strong>Imitation Learning from Demonstrations</strong>: Robots learn by observing human teleoperation or pre-recorded expert trajectories, often using paired video/language commands.</li><li><strong>Generative AI for Actions</strong>: Using generative models to synthesize action plans or trajectories that fulfill a given multimodal prompt.</li></ul><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># Conceptual Python snippet: VLA Model inferring action</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># This is highly abstract, as real VLA models are complex neural networks</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">class</span><span class="token plain"> </span><span class="token class-name">VLAModel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">__init__</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> language_encoder</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> vision_encoder</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> action_decoder</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">lang_enc </span><span class="token operator">=</span><span class="token plain"> language_encoder</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">vis_enc </span><span class="token operator">=</span><span class="token plain"> vision_encoder</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">action_dec </span><span class="token operator">=</span><span class="token plain"> action_decoder</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">infer_action</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> text_command</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> image_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token comment" style="color:rgb(98, 114, 164)"># 1. Encode language and vision</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        lang_features </span><span class="token operator">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">lang_enc</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">encode</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">text_command</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        vis_features </span><span class="token operator">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">vis_enc</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">encode</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">image_data</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token comment" style="color:rgb(98, 114, 164)"># 2. Fuse features (e.g., concatenate, cross-attention)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        fused_features </span><span class="token operator">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">fuse</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">lang_features</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> vis_features</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token comment" style="color:rgb(98, 114, 164)"># 3. Decode into action</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        action </span><span class="token operator">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">action_dec</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">decode</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">fused_features</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">return</span><span class="token plain"> action</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Example usage (simplified)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># vla_model = VLAModel(...)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># command = &quot;pick up the red apple&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># current_image = robot_camera.get_image()</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># robot_action = vla_model.infer_action(command, current_image)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># robot_controller.execute(robot_action)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><em>Code 10.1: Highly conceptual Python representation of a VLA model&#x27;s inference flow.</em></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="applications-in-humanoid-robotics">Applications in Humanoid Robotics<a href="#applications-in-humanoid-robotics" class="hash-link" aria-label="Direct link to Applications in Humanoid Robotics" title="Direct link to Applications in Humanoid Robotics">​</a></h2><p>VLA models unlock a new level of interactivity and autonomy for humanoids.</p><ul><li><strong>Instruction Following</strong>: Humanoids can understand and execute complex commands like &quot;Go to the kitchen, find the milk in the fridge, and bring it to the table.&quot;</li><li><strong>Task Automation</strong>: Automating multi-step tasks that require visual perception and adaptable manipulation.</li><li><strong>Human-Robot Collaboration</strong>: Enabling seamless collaboration where humans can intuitively direct robots using natural language.</li><li><strong>Exploration and Search</strong>: Directing robots to &quot;find something interesting in the corner&quot; or &quot;search for a specific tool.&quot;</li><li><strong>Personal Assistants</strong>: Humanoid robots acting as intelligent companions, responding to verbal requests that involve physical interaction with the environment.</li></ul><div class="language-mermaid codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-mermaid codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sequenceDiagram</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    participant User</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    participant HumanoidVLA</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    participant RobotController</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    participant Environment</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    User-&gt;&gt;HumanoidVLA: &quot;Pick up the blue box&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    HumanoidVLA-&gt;&gt;HumanoidVLA: Process (Language + Vision)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    HumanoidVLA-&gt;&gt;Environment: Perceive (Identify blue box)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    HumanoidVLA-&gt;&gt;RobotController: Plan Gripper Pose, Arm Trajectory</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    RobotController-&gt;&gt;Environment: Execute Manipulation</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    Environment-&gt;&gt;HumanoidVLA: Feedback (Success/Failure)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    HumanoidVLA-&gt;&gt;User: &quot;I have picked up the blue box.&quot;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><em>Figure 10.2: Humanoid robot executing a vision-language instruction.</em></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="challenges-and-future-directions">Challenges and Future Directions<a href="#challenges-and-future-directions" class="hash-link" aria-label="Direct link to Challenges and Future Directions" title="Direct link to Challenges and Future Directions">​</a></h2><p>Despite their promise, VLA models face several challenges:</p><ul><li><strong>Data Scarcity</strong>: Acquiring massive, high-quality, multimodal datasets (vision-language-action triplets) is difficult and expensive.</li><li><strong>Generalization</strong>: VLA models often struggle to generalize to novel objects, environments, or tasks not seen during training.</li><li><strong>Robustness</strong>: Performance can degrade significantly in cluttered scenes, poor lighting, or with ambiguous commands.</li><li><strong>Safety and Explainability</strong>: Ensuring safe action generation and understanding the reasoning behind a robot&#x27;s decisions are crucial.</li></ul><p>Future work focuses on improving data efficiency (e.g., self-supervised learning, synthetic data), enhancing generalization capabilities (e.g., foundation models for robotics), and developing more sophisticated reasoning and planning components that can integrate with VLA models.</p><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-takeaways">Key Takeaways<a href="#key-takeaways" class="hash-link" aria-label="Direct link to Key Takeaways" title="Direct link to Key Takeaways">​</a></h2><ul><li>Vision-Language-Action (VLA) models enable robots to understand and execute natural language commands based on visual perception.</li><li>They fuse capabilities from Large Language Models (LLMs) and Large Vision Models (LVMs).</li><li>Multimodal fusion techniques combine information from different modalities.</li><li>Translating abstract commands to low-level robot actions is a core challenge, often using reinforcement or imitation learning.</li><li>VLA models enable advanced applications like instruction following and human-robot collaboration for humanoids.</li><li>Challenges include data scarcity, generalization, robustness, safety, and explainability.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="practice-assignment">Practice Assignment<a href="#practice-assignment" class="hash-link" aria-label="Direct link to Practice Assignment" title="Direct link to Practice Assignment">​</a></h2><ol><li>Imagine a VLA-powered humanoid robot in a kitchen. Provide three distinct natural language commands, each requiring a different combination of visual perception, language understanding, and physical action (e.g., finding an object, preparing a simple dish, cleaning up).</li><li>Research a recent breakthrough in VLA models for robotics (e.g., Google&#x27;s PaLM-E, Open AI&#x27;s DALL-E/CLIP variants applied to robotics). Summarize its architecture, key contributions, and how it addresses some of the challenges mentioned in this chapter.</li><li>Discuss how a VLA model could be used to improve the human-robot collaboration aspect of a humanoid robot assisting an elderly person at home. What kind of interactions would be possible, and what ethical considerations arise?</li></ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/10-vision-language-action-models.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/navigation-and-bipedal-locomotion"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Navigation and Bipedal Locomotion</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/conversational-robotics-voice-to-action"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Conversational Robotics - Voice to Action</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#bridging-perception-cognition-and-action-with-language" class="table-of-contents__link toc-highlight">Bridging Perception, Cognition, and Action with Language</a><ul><li><a href="#the-multimodal-challenge-in-robotics" class="table-of-contents__link toc-highlight">The Multimodal Challenge in Robotics</a></li></ul></li><li><a href="#foundations-of-vla-models" class="table-of-contents__link toc-highlight">Foundations of VLA Models</a><ul><li><a href="#101-large-language-models-llms" class="table-of-contents__link toc-highlight">10.1. Large Language Models (LLMs)</a></li><li><a href="#102-large-vision-models-lvms" class="table-of-contents__link toc-highlight">10.2. Large Vision Models (LVMs)</a></li><li><a href="#103-multimodal-fusion" class="table-of-contents__link toc-highlight">10.3. Multimodal Fusion</a></li></ul></li><li><a href="#from-language-to-robot-action" class="table-of-contents__link toc-highlight">From Language to Robot Action</a><ul><li><a href="#104-action-space-and-representation" class="table-of-contents__link toc-highlight">10.4. Action Space and Representation</a></li><li><a href="#105-learning-paradigms" class="table-of-contents__link toc-highlight">10.5. Learning Paradigms</a></li></ul></li><li><a href="#applications-in-humanoid-robotics" class="table-of-contents__link toc-highlight">Applications in Humanoid Robotics</a></li><li><a href="#challenges-and-future-directions" class="table-of-contents__link toc-highlight">Challenges and Future Directions</a></li><li><a href="#key-takeaways" class="table-of-contents__link toc-highlight">Key Takeaways</a></li><li><a href="#practice-assignment" class="table-of-contents__link toc-highlight">Practice Assignment</a></li></ul></div></div></div></div></main></div></div><button class="chatButton_gSQt" aria-label="Toggle chat"><svg width="28" height="28" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z" stroke-linecap="round" stroke-linejoin="round"></path><circle cx="9" cy="10" r="0.5" fill="currentColor" stroke-width="0"></circle><circle cx="12" cy="10" r="0.5" fill="currentColor" stroke-width="0"></circle><circle cx="15" cy="10" r="0.5" fill="currentColor" stroke-width="0"></circle></svg></button></div>
<script src="/assets/js/runtime~main.a22e79de.js"></script>
<script src="/assets/js/main.8e1e1db1.js"></script>
</body>
</html>